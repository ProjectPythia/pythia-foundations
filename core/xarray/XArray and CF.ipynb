{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XArray & CF Introduction\n",
    "\n",
    "<div style=\"float:right; width:250 px\"><img src=\"http://xarray.pydata.org/en/stable/_static/dataset-diagram-logo.png\" alt=\"xarray Logo\" style=\"height: 250px;\"></div>\n",
    "\n",
    "### Questions\n",
    "1. What is XArray?\n",
    "2. How does XArray fit in with Numpy and Pandas?\n",
    "3. What is the CF convention and how do we use it with Xarray?\n",
    "\n",
    "### Objectives\n",
    "1. Create a `DataArray`.\n",
    "2. Open netCDF data using XArray\n",
    "3. Subset the data.\n",
    "4. Write a CF-compliant netCDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XArray\n",
    "\n",
    "XArray expands on the capabilities on NumPy arrays, providing a lot of streamlined data manipulation. It is similar in that respect to Pandas, but whereas Pandas excels at working with tabular data, XArray is focused on N-dimensional arrays of data (i.e. grids). Its interface is based largely on the netCDF data model (variables, attributes, and dimensions), but it goes beyond the traditional netCDF interfaces to provide functionality similar to netCDF-java's Common Data Model (CDM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataArray`\n",
    "\n",
    "The `DataArray` is one of the basic building blocks of XArray. It provides a NumPy ndarray-like object that expands to provide two critical pieces of functionality:\n",
    "\n",
    "1. Coordinate names and values are stored with the data, making slicing and indexing much more powerful\n",
    "2. It has a built-in container for attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Convention for import to get shortened namespace\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create some sample \"temperature\" data\n",
    "data = 283 + 5 * np.random.randn(5, 3, 4)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a basic `DataArray` by passing it just a numpy array of random data. Note that XArray generates some basic dimension names for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = xr.DataArray(data)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass in our own dimension names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = xr.DataArray(data, dims=['time', 'lat', 'lon'])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already improved upon from a numpy array, because we have names for each of the dimensions (or axes in NumPy parlance). Even better, we can take arrays representing the values for the coordinates for each of these dimensions and associate them with the data when we create the `DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to create an array of datetimes\n",
    "import pandas as pd\n",
    "times = pd.date_range('2018-01-01', periods=5)\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample lon/lats\n",
    "lons = np.linspace(-120, -60, 4)\n",
    "lats = np.linspace(25, 55, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create the `DataArray` instance, we pass in the arrays we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = xr.DataArray(data, coords=[times, lats, lons], dims=['time', 'lat', 'lon'])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we can also set some attribute metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.attrs['units'] = 'kelvin'\n",
    "temp.attrs['standard_name'] = 'air_temperature'\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happens if we perform a mathematical operaton with the `DataArray`: the coordinate values persist, but the attributes are lost. This is done because it is very challenging to know if the attribute metadata is still correct or appropriate after arbitrary arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, convert Kelvin to Celsius\n",
    "temp - 273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection\n",
    "We can use the `.sel` method to select portions of our data based on these coordinate values, rather than using indices (this is similar to the CDM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sel(time='2018-01-02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.sel` has the flexibility to also perform nearest neighbor sampling, taking an optional tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "temp.sel(time='2018-01-07', method='nearest', tolerance=timedelta(days=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.interp` ([docs](http://xarray.pydata.org/en/stable/interpolation.html)) works similarly to `.sel()`. Using `.interp()`, we can get an interpolated time series \"forecast\" for Boulder (40°N, 105°W) or your favorite latitude/longitude location. Note, xarray's interpolation functionality requires [SciPy](https://scipy.org/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.interp(lon=-105, lat=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing with Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sel(time=slice('2018-01-01', '2018-01-03'), lon=slice(-110, -70), lat=slice(25, 45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.loc`\n",
    "\n",
    "All of these operations can also be done within square brackets on the `.loc` attribute of the `DataArray`. This permits a much more numpy-looking syntax, though you lose the ability to specify the names of the various dimensions. Instead, the slicing must be done in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As done above\n",
    "temp.loc['2018-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc['2018-01-01':'2018-01-03', 25:45, -110:-70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This *doesn't* work however:\n",
    "#temp.loc[-110:-70, 25:45,'2018-01-01':'2018-01-03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Opening netCDF data\n",
    "With its close ties to the netCDF data model, XArray also supports netCDF as a first-class file format. This means it has easy support for opening netCDF datasets, so long as they conform to some of XArray's limitations (such as 1-dimensional coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open sample North American Reanalysis data in netCDF format\n",
    "ds = xr.open_dataset('NARR_19930313_0000.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `Dataset` object, which is a container that contains one or more `DataArray`s, which can also optionally share coordinates. We can then pull out individual fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isobaric1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['isobaric1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset`s also support much of the same subsetting operations as `DataArray`, but will perform the operation on all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1000 = ds.sel(isobaric1=1000.0)\n",
    "ds_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1000.Temperature_isobaric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation operations\n",
    "\n",
    "Not only can you use the named dimensions for manual slicing and indexing of data, but you can also use it to control aggregation operations, like `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_winds = ds['u-component_of_wind_isobaric']\n",
    "u_winds.std(dim=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "\n",
    "Using the sample dataset, calculate the mean temperature profile (temperature as a function of pressure) over Colorado within this dataset. For this exercise, consider the bounds of Colorado to be:\n",
    "     <ul>\n",
    "         <li>x: -182km to 424km</li>\n",
    "         <li>y: -1450km to -990km</li>\n",
    "    </ul>\n",
    "    \n",
    "(37°N to 41°N and 102°W to 109°W projected to Lambert Conformal projection coordinates)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>SOLUTION</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mean_profile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sample dataset, we can calculate the mean temperature profile (temperature as a function of pressure) over Colorado within this dataset. For this exercise, consider the bounds of Colorado to be:\n",
    " * x: -182km to 424km\n",
    " * y: -1450km to -990km\n",
    "    \n",
    "(37°N to 41°N and 102°W to 109°W projected to Lambert Conformal projection coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = ds.Temperature_isobaric\n",
    "co_temps = temps.sel(x=slice(-182, 424), y=slice(-1450, -990))\n",
    "prof = co_temps.mean(dim=['x', 'y'])\n",
    "prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "There is much more in the XArray library. To learn more, visit the [XArray Documentation](http://xarray.pydata.org/en/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Climate and Forecasting Metadata Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better enable reproducible data and research, the Climate and Forecasting (CF) metadata convention was created to have proper metadata in atmospheric data files. In the remainder of this notebook, we will introduce the CF data model and discuss some netCDF implementation details to consider when deciding how to write data with CF and netCDF. We will cover gridded data in this notebook, with more in depth examples provided in the full [CF notebook](https://github.com/Unidata/python-workshop/blob/master/notebooks/CF%20Conventions/NetCDF%20and%20CF%20-%20The%20Basics.ipynb). Xarray makes the creation of netCDFs with proper metadata simple and straightforward, so we will use that, instead of the netCDF-Python library.\n",
    "\n",
    "This assumes a basic understanding of netCDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"gridded\"></a>\n",
    "## Gridded Data\n",
    "Let's say we're working with some numerical weather forecast model output. Let's walk through the steps necessary to store this data in netCDF, using the Climate and Forecasting metadata conventions to ensure that our data are available to as many tools as possible.\n",
    "\n",
    "To start, let's assume the following about our data:\n",
    "* It corresponds to forecast three dimensional temperature at several times\n",
    "* The native coordinate system of the model is on a regular grid that represents the Earth on a Lambert conformal projection.\n",
    "\n",
    "We'll also go ahead and generate some arrays of data below to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful Python tools\n",
    "from datetime import datetime\n",
    "\n",
    "# Twelve hours of hourly output starting at 22Z today\n",
    "start = datetime.utcnow().replace(hour=22, minute=0, second=0, microsecond=0)\n",
    "times = np.array([start + timedelta(hours=h) for h in range(13)])\n",
    "\n",
    "# 3km spacing in x and y\n",
    "x = np.arange(-150, 153, 3)\n",
    "y = np.arange(-100, 100, 3)\n",
    "\n",
    "# Standard pressure levels in hPa\n",
    "press = np.array([1000, 925, 850, 700, 500, 300, 250])\n",
    "\n",
    "temps = np.random.randn(times.size, press.size, y.size, x.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time coordinates must contain a `units` attribute with a string value with a form similar to `'seconds since 2019-01-06 12:00:00.00'`. 'seconds', 'minutes', 'hours', and 'days' are the most commonly used units for time. Due to the variable length of months and years, they are not recommended.\n",
    "\n",
    "Before we can write data, we need to first need to convert our list of Python `datetime` instances to numeric values. We can use the `cftime` library to make this easy to convert using the unit string as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cftime import date2num\n",
    "time_units = 'hours since {:%Y-%m-%d 00:00}'.format(times[0])\n",
    "time_vals = date2num(times, time_units)\n",
    "time_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the `forecast_time` variable just as we did before for the other coordinate variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert arrays into Xarray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({'temperature': (['time', 'z', 'y', 'x'], temps, {'units':'Kelvin'})},\n",
    "                 coords={'x_dist': (['x'], x, {'units':'km'}),\n",
    "                         'y_dist': (['y'], y, {'units':'km'}),\n",
    "                         'pressure': (['z'], press, {'units':'hPa'}),\n",
    "                         'forecast_time': (['time'], times)\n",
    "                })\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to how xarray handles time units, we need to encode the units in the `forecast_time` coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.forecast_time.encoding['units'] = time_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our data variable, we can see the units printed out, so they were attached properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by adding some global attribute metadata. These are recommendations from the standard (not required), but they're easy to add and help users keep the data straight, so let's go ahead and do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs['Conventions'] = 'CF-1.7'\n",
    "ds.attrs['title'] = 'Forecast model run'\n",
    "ds.attrs['nc.institution'] = 'Unidata'\n",
    "ds.attrs['source'] = 'WRF-1.5'\n",
    "ds.attrs['history'] = str(datetime.utcnow()) + ' Python'\n",
    "ds.attrs['references'] = ''\n",
    "ds.attrs['comment'] = ''\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add attributes to this variable to define metadata. The CF conventions require a `units` attribute to be set for all variables that represent a dimensional quantity. The value of this attribute needs to be parsable by the UDUNITS library. Here we have already set it to a value of `'Kelvin'`. We also set the standard (optional) attributes of `long_name` and `standard_name`. The former contains a longer description of the variable, while the latter comes from a controlled vocabulary in the CF conventions. This allows users of data to understand, in a standard fashion, what a variable represents. If we had missing values, we could also set the `missing_value` attribute to an appropriate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NASA Dataset Interoperability Recommendations:**\n",
    ">\n",
    "> Section 2.2 - Include Basic CF Attributes\n",
    ">\n",
    "> Include where applicable: `units`, `long_name`, `standard_name`, `valid_min` / `valid_max`, `scale_factor` / `add_offset` and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.temperature.attrs['standard_name'] = 'air_temperature'\n",
    "ds.temperature.attrs['long_name'] = 'Forecast air temperature'\n",
    "ds.temperature.attrs['missing_value'] = -9999\n",
    "ds.temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly orient our data in time and space, we need to go beyond dimensions (which define common sizes and alignment) and include values along these dimensions, which are called \"Coordinate Variables\". Generally, these are defined by creating a one dimensional variable with the same name as the respective dimension.\n",
    "\n",
    "To start, we define variables which define our `x` and `y` coordinate values. These variables include `standard_name`s which allow associating them with projections (more on this later) as well as an optional `axis` attribute to make clear what standard direction this coordinate refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.x.attrs['axis'] = 'X' # Optional\n",
    "ds.x.attrs['standard_name'] = 'projection_x_coordinate'\n",
    "ds.x.attrs['long_name'] = 'x-coordinate in projected coordinate system'\n",
    "\n",
    "ds.y.attrs['axis'] = 'Y' # Optional\n",
    "ds.y.attrs['standard_name'] = 'projection_y_coordinate'\n",
    "ds.y.attrs['long_name'] = 'y-coordinate in projected coordinate system'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a coordinate variable `pressure` to reference our data in the vertical dimension. The `standard_name` of `'air_pressure'` is sufficient to identify this coordinate variable as the vertical axis, but let's go ahead and specify the `axis` as well. We also specify the attribute `positive` to indicate whether the variable increases when going up or down. In the case of pressure, this is technically optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.pressure.attrs['axis'] = 'Z'  # Optional\n",
    "ds.pressure.attrs['standard_name'] = 'air_pressure'\n",
    "ds.pressure.attrs['positive'] = 'down'  # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.forecast_time['axis'] = 'T'  # Optional\n",
    "ds.forecast_time['standard_name'] = 'time'  # Optional\n",
    "ds.forecast_time['long_name'] = 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilliary Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are still not CF-compliant because they do not contain latitude and longitude information, which is needed to properly locate the data. To solve this, we need to add variables with latitude and longitude. These are called \"auxillary coordinate variables\", not because they are extra, but because they are not simple one dimensional variables.\n",
    "\n",
    "Below, we first generate longitude and latitude values from our projected coordinates using the `pyproj` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj\n",
    "X, Y = np.meshgrid(x, y)\n",
    "lcc = Proj({'proj':'lcc', 'lon_0':-105, 'lat_0':40, 'a':6371000.,\n",
    "            'lat_1':25})\n",
    "lon, lat = lcc(X * 1000, Y * 1000, inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the needed variables. Both are dimensioned on `y` and `x` and are two-dimensional. The longitude variable is identified as actually containing such information by its required units of `'degrees_east'`, as well as the optional `'longitude'` `standard_name` attribute. The case is the same for latitude, except the units are `'degrees_north'` and the `standard_name` is `'latitude'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign_coords(lon = (['y', 'x'], lon))\n",
    "ds = ds.assign_coords(lat = (['y', 'x'], lat))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lon.attrs['units'] = 'degrees_east'\n",
    "ds.lon.attrs['standard_name'] = 'longitude'  # Optional\n",
    "ds.lon.attrs['long_name'] = 'longitude'\n",
    "\n",
    "ds.lat.attrs['units'] = 'degrees_north'\n",
    "ds.lat.attrs['standard_name'] = 'latitude'  # Optional\n",
    "ds.lat.attrs['long_name'] = 'latitude'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the variables created, we identify these variables as containing coordinates for the `Temperature` variable by setting the `coordinates` value to a space-separated list of the names of the auxilliary coordinate variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate System Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data specified on a Lambert conformal projected grid, it would be good to include this information in our metadata. We can do this using a \"grid mapping\" variable. This uses a dummy scalar variable as a namespace for holding all of the required information. Relevant variables then reference the dummy variable with their `grid_mapping` attribute.\n",
    "\n",
    "Below we create a variable and set it up for a Lambert conformal conic projection on a spherical earth. The `grid_mapping_name` attribute describes which of the CF-supported grid mappings we are specifying. The names of additional attributes vary between the mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lambert_projection'] = int()\n",
    "ds.lambert_projection.attrs['grid_mapping_name'] = 'lambert_conformal_conic'\n",
    "ds.lambert_projection.attrs['standard_parallel'] = 25.\n",
    "ds.lambert_projection.attrs['latitude_of_projection_origin'] = 40.\n",
    "ds.lambert_projection.attrs['longitude_of_central_meridian'] = -105.\n",
    "ds.lambert_projection.attrs['semi_major_axis'] = 6371000.0\n",
    "ds.lambert_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created the variable, all that's left is to set the `grid_mapping` attribute on our `Temperature` variable to the name of our dummy variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.temperature.attrs['grid_mapping'] = 'lambert_projection'  # or proj_var.name\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray has built-in support for a few flavors of netCDF. Here we'll write a netCDF4 file from our Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('test_netcdf.nc', format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncdump test_netcdf.nc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
